{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install robust_minisets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10, CIFAR100\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import medmnist\n",
    "from medmnist import INFO as INFO_MEDMNIST\n",
    "\n",
    "import robust_minisets\n",
    "from robust_minisets import INFO as INFO_ROBUST\n",
    "from robust_minisets.info import DEFAULT_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Robust-Minisets v{robust_minisets.__version__} @ {robust_minisets.HOMEPAGE}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We showcase the work with an example training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flag_dict = {\n",
    "    \"cifar-10-1\"        : (CIFAR10, getattr(robust_minisets, \"CIFAR10_1\"), None, INFO_ROBUST[\"cifar-10-1\"]),\n",
    "    \"cifar-10-c\"        : (CIFAR10, getattr(robust_minisets, \"CIFAR10C\"), None, INFO_ROBUST[\"cifar-10-c\"]),\n",
    "    \"cifar-100-c\"       : (CIFAR100, getattr(robust_minisets, \"CIFAR100C\"), None, INFO_ROBUST[\"cifar-100-c\"]),\n",
    "    \"eurosat\"           : (getattr(robust_minisets, \"EuroSAT\"), getattr(robust_minisets, \"EuroSAT\"), INFO_ROBUST[\"eurosat\"], INFO_ROBUST[\"eurosat\"]),\n",
    "    \"eurosat-c\"         : (getattr(robust_minisets, \"EuroSAT\"), getattr(robust_minisets, \"EuroSATC\"), INFO_ROBUST[\"eurosat\"], INFO_ROBUST[\"eurosat-c\"]),\n",
    "    \"bloodmnist-c\"      : (getattr(medmnist, \"BloodMNIST\"), getattr(robust_minisets, \"BloodMNISTC\"), INFO_MEDMNIST[\"bloodmnist\"], INFO_ROBUST[\"bloodmnist-c\"]),\n",
    "    \"breastmnist-c\"     : (getattr(medmnist, \"BreastMNIST\"), getattr(robust_minisets, \"BreastMNISTC\"), INFO_MEDMNIST[\"breastmnist\"], INFO_ROBUST[\"breastmnist-c\"]),\n",
    "    \"dermamnist-c\"      : (getattr(medmnist, \"DermaMNIST\"), getattr(robust_minisets, \"DermaMNISTC\"), INFO_MEDMNIST[\"dermamnist\"], INFO_ROBUST[\"dermamnist-c\"]),\n",
    "    \"octmnist-c\"        : (getattr(medmnist, \"OCTMNIST\"), getattr(robust_minisets, \"OCTMNISTC\"), INFO_MEDMNIST[\"octmnist\"], INFO_ROBUST[\"octmnist-c\"]),\n",
    "    \"organamnist-c\"     : (getattr(medmnist, \"OrganAMNIST\"), getattr(robust_minisets, \"OrganAMNISTC\"), INFO_MEDMNIST[\"organamnist\"], INFO_ROBUST[\"organamnist-c\"]),\n",
    "    \"organcmnist-c\"     : (getattr(medmnist, \"OrganCMNIST\"), getattr(robust_minisets, \"OrganCMNISTC\"), INFO_MEDMNIST[\"organcmnist\"], INFO_ROBUST[\"organcmnist-c\"]),\n",
    "    \"organsmnist-c\"     : (getattr(medmnist, \"OrganSMNIST\"), getattr(robust_minisets, \"OrganSMNISTC\"), INFO_MEDMNIST[\"organsmnist\"], INFO_ROBUST[\"organsmnist-c\"]),\n",
    "    \"pathmnist-c\"       : (getattr(medmnist, \"PathMNIST\"), getattr(robust_minisets, \"PathMNISTC\"), INFO_MEDMNIST[\"pathmnist\"], INFO_ROBUST[\"pathmnist-c\"]),\n",
    "    \"pneumoniamnist-c\"  : (getattr(medmnist, \"PneumoniaMNIST\"), getattr(robust_minisets, \"PneumoniaMNISTC\"), INFO_MEDMNIST[\"pneumoniamnist\"], INFO_ROBUST[\"pneumoniamnist-c\"]),\n",
    "    \"tissuemnist-c\"     : (getattr(medmnist, \"TissueMNIST\"), getattr(robust_minisets, \"TissueMNISTC\"), INFO_MEDMNIST[\"tissuemnist\"], INFO_ROBUST[\"tissuemnist-c\"]),\n",
    "    \"tiny-imagenet\"     : (getattr(robust_minisets, \"TinyImageNet\"), getattr(robust_minisets, \"TinyImageNet\"), INFO_ROBUST[\"tiny-imagenet\"], INFO_ROBUST[\"tiny-imagenet\"]), \n",
    "    \"tiny-imagenet-a\"   : (getattr(robust_minisets, \"TinyImageNet\"), getattr(robust_minisets, \"TinyImageNetA\"), INFO_ROBUST[\"tiny-imagenet\"], INFO_ROBUST[\"tiny-imagenet-a\"]),\n",
    "    \"tiny-imagenet-c\"   : (getattr(robust_minisets, \"TinyImageNet\"), getattr(robust_minisets, \"TinyImageNetC\"), INFO_ROBUST[\"tiny-imagenet\"], INFO_ROBUST[\"tiny-imagenet-c\"]),\n",
    "    \"tiny-imagenet-r\"   : (getattr(robust_minisets, \"TinyImageNet\"), getattr(robust_minisets, \"TinyImageNetR\"), INFO_ROBUST[\"tiny-imagenet\"], INFO_ROBUST[\"tiny-imagenet-r\"]),\n",
    "    \"tiny-imagenet-v2\"  : (getattr(robust_minisets, \"TinyImageNet\"), getattr(robust_minisets, \"TinyImageNetv2\"), INFO_ROBUST[\"tiny-imagenet\"], INFO_ROBUST[\"tiny-imagenet-v2\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flag = 'tiny-imagenet-r'\n",
    "\n",
    "download = True\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 128\n",
    "lr = 0.001\n",
    "\n",
    "DataClass, DataClass_Robust, info_train_test, info_robust = data_flag_dict[data_flag]\n",
    "\n",
    "# Info train/test\n",
    "if \"cifar\" in data_flag:\n",
    "    n_channels_train_test = 3\n",
    "else:\n",
    "    n_channels_train_test = info_train_test[\"n_channels\"]\n",
    "\n",
    "# Robust-Datasets info\n",
    "n_channels_robust = info_robust[\"n_channels\"]\n",
    "n_classes = info_robust[\"n_classes\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, we read the train, test and robustness data, preprocess them and encapsulate them into dataloader form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing of data\n",
    "train_test_transform = [\n",
    "    transforms.Resize(28),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[.5], std=[.5])\n",
    "]\n",
    "\n",
    "robust_transform = [\n",
    "    transforms.Resize(28),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[.5], std=[.5])\n",
    "]\n",
    "\n",
    "if n_channels_train_test < 3:\n",
    "        train_test_transform  += [\n",
    "            transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "        ]\n",
    "\n",
    "if n_channels_robust < 3:\n",
    "        robust_transform  += [\n",
    "            transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "        ]\n",
    "\n",
    "train_test_transform = transforms.Compose(train_test_transform)\n",
    "robust_transform = transforms.Compose(robust_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "if \"cifar\" in data_flag:\n",
    "    train_dataset = DataClass(root=DEFAULT_ROOT, train=True, transform=train_test_transform, download=download)\n",
    "    test_dataset = DataClass(root=DEFAULT_ROOT, train=False, transform=train_test_transform, download=download)\n",
    "else:\n",
    "    train_dataset = DataClass(split=\"train\", transform=train_test_transform, download=download)\n",
    "    test_dataset = DataClass(split=\"test\", transform=train_test_transform, download=download)\n",
    "test_robust_dataset = DataClass_Robust(split=\"test\", transform=robust_transform, download=download)\n",
    "\n",
    "# encapsulate data into dataloader form\n",
    "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader_at_eval = data.DataLoader(dataset=train_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
    "test_loader = data.DataLoader(dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
    "test_robust_loader = data.DataLoader(dataset=test_robust_dataset, batch_size=2*BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_robust_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "test_robust_dataset.montage(length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# montage\n",
    "test_robust_dataset.montage(length=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then, we define a simple model for illustration, object function and optimizer that we use to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simple CNN model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 4 * 4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = Net(in_channels=3, num_classes=n_classes)\n",
    "    \n",
    "# define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, we can start to train and evaluate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    for inputs, targets in tqdm(train_loader):\n",
    "        # forward + backward + optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        targets = targets.squeeze().long()\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "def test(split):\n",
    "    model.eval()\n",
    "    y_true = torch.tensor([])\n",
    "    y_score = torch.tensor([])\n",
    "\n",
    "    data_loader_dict = {\n",
    "        \"train\" : train_loader_at_eval,\n",
    "        \"test\"  : test_loader,\n",
    "        \"test_robust\": test_robust_loader\n",
    "    }\n",
    "    \n",
    "    data_loader = data_loader_dict[split]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            targets = targets.squeeze().long()\n",
    "            outputs = outputs.softmax(dim=-1)\n",
    "            targets = targets.float().resize_(len(targets), 1)\n",
    "\n",
    "            y_true = torch.cat((y_true, targets), 0)\n",
    "            y_score = torch.cat((y_score, outputs), 0)\n",
    "\n",
    "        y_true = y_true.numpy().squeeze()\n",
    "        y_score = y_score.detach().numpy().squeeze()\n",
    "\n",
    "        accuracy = accuracy_score(y_true, np.argmax(y_score, axis=-1))\n",
    "    \n",
    "        print(f\"Split: {split} | Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "        \n",
    "print('==> Evaluating ...')\n",
    "test('train')\n",
    "test('test')\n",
    "test('test_robust')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
